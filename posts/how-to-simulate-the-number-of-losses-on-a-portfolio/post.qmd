---
title: "How to Simulate the Number of Losses on a Portfolio?"
date: 2023-11-15
categories: ['Simulations', 'Frequency', 'Trend']
editor_options: 
  chunk_output_type: console
---

In a [previous post](../how-to-simulate-a-portfolio/post.html) we simulated a portfolio
of insurance policies. Today, we simulate its loss count.

We will need the portfolio data frame `policy_df` that we generated back then.
With the below, we re-run that code.

```{r}
#| output: false
library(tidyverse)
library(knitr)
library(RCurl)
url <- "https://api.github.com/repos/dreanod/actuarial-recipes/contents/posts/how-to-simulate-a-portfolio/post.qmd"
header <- c(
  Accept = 'application/vnd.github.v3.raw',
  'User-Agent' = 'curl'
)
source(purl(text = getURL(url, httpheader = header), output = tempfile()))
```

We can be interested in either bulk simulations of the total number of
losses over a period of time, or in how many losses have been generated
by each policy of the portfolio. We will look at both situations in this
post.

# Choice of Frequency Distribution

I will assume that the loss count for each policy is independent,
Poisson distributed. This implicitly assumes that the timing of losses
is independent[^1]. In other words, the fact that a policy had a loss at a given
time does not change the distribution of future losses (for this policy and for
the rest of the portfolio). This intuitively makes sense and is mostly in
line with real data [^2].

[^1]: This is linked to the memory-free property of the exponential distribution.
  But this is a topic for another post.

[^2]: Well, in many situations the loss distribution is slightly
  "over-dispersed" and one can adjust the Poisson distribution or use
  another one to reflect this. This is again a topic for another post.
  In most cases however, the Poisson distribution is just fine as a first
  approximation.

I will assume that **the frequency distribution for policies written on January
1, 2010 is 5.87%**. This means that one exposure written at this date will inccur
on average 0.0587 losses in a year. We further assume that the frequency
decreases at an annual rate of 1%.

```{r}
initial_freq <- 0.0587
freq_trend <- -1 / 100
```

# Bulk Simulation

In a bulk simulation we simulate the number of losses for a cohort of exposures
over period of time. In this case, we will look at the number of losses
generated by the policies written in each year.

## Calculation of the Annual Loss Frequencies

The total claim count for a group of policy is Poisson distributed with
frequency the sum of the frequencies of the individual policies. 
This is because the claim counts of each policy is Poisson and independent from
one another. For simplicity, we assume that the frequency is the same for all
policies written in the same year. As a result, the claim count of any given
policy year is proportional to the number of written exposures.

We therefore calculate the number of written exposure per year:

```{r}
freq_df <- policy_df |>
  group_by(policy_year = year(inception_date)) |>
  summarize(n_written_expo = sum(n_expo))
```

```{r}
#| code-fold: true
#| fig-cap: "Number of Exposures Written per Year"
display_table(freq_df)
```

Now, we can calculate the frequency for exposures written in
each annual cohort. The frequency needs to be trended from the
January 1, 2010 to the average inception date of each cohort:

```{r}
initial_freq_date <- ymd("2010-01-01")

freq_df <- policy_df |>
  group_by(PY = year(inception_date)) |>
  summarize(n_expo = sum(n_expo),
            mean_incept_dt = mean(inception_date),
            trending_period = (initial_freq_date %--% mean_incept_dt) / years(1),
            trending_factor = (1 + freq_trend)^trending_period,
            freq_per_expo = initial_freq * trending_factor,
            total_freq = freq_per_expo * n_expo)
```

```{r}
#| code-fold: true
#| fig-cap: "Calculation of the Annual Aggregate Frequency of Loss"
display_table(freq_df)
```

## Simulating the Number of Losses

We can now simulate the number of losses for each year:

```{r}
set.seed(100)    # <1>
freq_df <- freq_df |> 
  mutate(loss_count = rpois(n(), lambda = total_freq))
```
1. When doing random simulations, this makes sure that we can reproduce the
   same random outputs every time we run the code.

```{r}
#| code-fold: true
#| fig-cap: "Simulated Loss Count per Policy Year"
freq_df |> 
  select(PY, total_freq, loss_count) |> 
  display_table()
```

## Overview of the Simulated Data

We can compare the expected frequency to the simulated total loss count:

```{r}
#| code-fold: true
#| fig-cap: "Comparing Expected Claim Frequency to Simulated Loss Count"
freq_df |> 
  rename(`Expected Frequency` = total_freq,
         `Simulated Loss Count` = loss_count) |>
  pivot_longer(c("Expected Frequency", "Simulated Loss Count")) |>
  ggplot(aes(PY, value, color = name)) +
  geom_line() + geom_point() +
  ylab("Loss Count") + xlab("Policy Year") +
  labs(color = "")
```

Our simulated loss count is close to the expected frequency. However,
the simulated number goes up and down around the expected frequency
and we cannot discern a downward trend.


This is due to the natural variability of the Poisson distribution. We can show
this by comparing the simulated loss count to the 90% confidence interval of
the distribution:

```{r}
#| fig-cap: "Comparing Simulated Loss Count to Confidence Intervals"
freq_df |>
  mutate(lower_bound = qpois(.05, total_freq), # <1>
         upper_bound = qpois(.95, total_freq)) |> # <1>
  select(PY, total_freq, lower_bound, upper_bound, loss_count) |>
  display_table()
```
1. `qpois` is the quantile function of the Poisson distribution.

We can see from the table that all simulated loss counts fall within
the 90\% confidence interval, which helps us validate our results.

# Policy-per-Policy Simulation

The previous simulation only tells us about the total number of losses
generated in each cohort. However, it does not tell **which policy or exposure
generated which loss**. This is limiting for some applications. For
example, we may want to have the size of losses vary depending on exposure
characteristics. In this case, we need to simulate a loss count per policy.
We see how to do this in the rest of this post:

## Calculate average frequency per Policy

First, we need to calculate the expected frequency of loss for each policy. We
do this by trending the initial frequency from January 1, 2010 to the inception
date of each policy:

```{r}
policy_df <- policy_df |>
  mutate(trend_period = (initial_freq_date %--% inception_date) / years(1),
         trend_factor = (1 + freq_trend)^trend_period,
         frequency = initial_freq * trend_factor )
```

```{r}
#| code-fold: true
#| fig-cap: "Frequency Calculation for the First Policy of Each Year"
policy_df |>
  group_by(policy_year = year(inception_date)) |>
  filter(row_number() == 1) |>
  ungroup() |>
  select(policy_id, inception_date, trend_period, trend_factor, frequency) |>
  display_table()
```

We can check from this table that we do have a -1\% change in frequency
between policies that are written one year apart.

## Simulate poisson for each policy

With the expected per-policy frequency, we can simulate the number of actual
losses, assuming independence and a Poisson distribution:

```{r}
policy_df <- policy_df |>
  mutate(n_claims = rpois(n(), frequency)) # <1>
```
1. `rpois` will simulate a Poisson random variable for each row in
   the `policy_df` data frame, with the corresponding frequency
   given by the `frequency` column.

```{r}
#| code-fold: true
#| fig-cap: "Simulated Claim Count for the first Policy of Each Year"
policy_df |>
  select(policy_id, inception_date, frequency, n_claims) |>
  filter(policy_id %in% first_policy_of_a_year) |>
  display_table()
```

As the frequency is around 5\% we expect most policies will have
no claim.

## Check results and compare

As a check, we compare these results to the bulk simulations above.
We therefore aggregate the policy table to obtain annual expected
frequencies and simulated claim counts:

```{r}
#| code-fold: true
#| fig-cap: "Simulated Claim Count for the first Policy of Each Year"
summary_df <- policy_df |>
  group_by(PY = year(inception_date)) |>
  summarize(
    total_expected_loss_count = sum(frequency),
    total_simulated_loss_count = sum(n_claims),
  )

display_table(summary_df)
```
We plot this data against the bulk data to help comparison:

```{r}
#| code-fold: true
#| fig-cap: "Comparing Bulk Annual to Per-Policy Frequency Simulations"
summary_df_per_policy <- summary_df |>
  rename(`Per-Policy Expected` = total_expected_loss_count,
         `Per-Policy Simulated` = total_simulated_loss_count) |>
  pivot_longer(c("Per-Policy Expected", "Per-Policy Simulated"))

summary_df_bulk <- freq_df |> 
  rename(`Bulk Expected` = total_freq,
         `Bulk Simulated` = loss_count) |>
  pivot_longer(c("Bulk Expected", "Bulk Simulated"))

summary_df <- bind_rows(summary_df_bulk, summary_df_per_policy)

summary_df |>
  ggplot(aes(PY, value, color = name)) +
  geom_line() + geom_point() +
  ylab("Loss Count") + xlab("Policy Year") +
  labs(color = "", subtitle = "*Bulk and per-policy expected are overlapping")
```

The bulk and the per-policy expected loss frequencies are so close that we
cannot distinguish them in the plot. This shows that both approach are
practically equivalent for simulating annual loss counts.

On the other hand, the simulated loss counts are different due to the natural
volatility of the Poisson distribution. We can however verify that the
per-policy simulated loss count is within the 90\% confidence interval we
derived for the bulk simulation.

# Conclusion

In summary, we have seen too approaches to simulate loss counts for
a portfolio of policies: a bulk approach that simulates claim counts
on annual cohorts of written policies, and a per-policy approach that
simulates the claim count for individual policies. The latter approach
is the most flexible, as it gives us the possibility to simulate a
dataset of individual losses, with claim amount and occurrence date.
We will explore this topic in a future post.

